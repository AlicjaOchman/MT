{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part consitutes the follwing steps:\n",
    "1. Searching multiple combinations of the data preprocessing methods and classifiers to find ones that seem to be the most suitable for this binary classification problem\n",
    "2. Optimization of the Support Vector Machines classifier\n",
    "3. Optimization of the Random Forests classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as seabornInstance \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data\n",
    "dataset = pd.read_csv(\"ultimateData.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup row names\n",
    "dataset.set_index('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['value'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create np arrays of the labels and features and divide the dataset into training and test set\n",
    "labels = dataset['value'].values\n",
    "features = dataset.drop('value', axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features[:,1:].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, precision_recall_curve, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import ensemble\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "import xgboost as xgb\n",
    "from imblearn.under_sampling import ClusterCentroids, RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tryout different preprocessing methods together with different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensionality_reductors = [PCA(n_components=0.95),PCA(n_components=0.90), KernelPCA(kernel = 'linear'), KernelPCA(kernel = 'poly', degree=3), KernelPCA(kernel = 'poly', degree=5), None]\n",
    "classifiers = [LogisticRegression(random_state=0, solver='lbfgs', max_iter=10000), RandomForestClassifier(n_estimators=1000), SVC(kernel='poly', degree = 3), SVC(kernel='poly', degree = 4), SVC(kernel='poly', degree = 5), xgb.XGBClassifier()]\n",
    "preprocess = [StandardScaler(), MinMaxScaler(), None]\n",
    "balancers = [SMOTE(), RandomOverSampler(), None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a sklearn pipeline to tryout the different combinations without doing consequitive steps by hand- fit the different models on the trainings set and predict on the test set\n",
    "Calculate appropraite metrics and put the results into the table for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = []\n",
    "for scaler in preprocess:\n",
    "    for dimensionality_reductor in dimensionality_reductors:\n",
    "        for balancer in balancers:\n",
    "            for classifier in classifiers:\n",
    "                pipeline = Pipeline([\n",
    "                    (type(scaler).__name__, scaler),\n",
    "                    (type(dimensionality_reductor).__name__, dimensionality_reductor),\n",
    "                    (type(balancer).__name__, balancer),(type(classifier).__name__, classifier)\n",
    "                    ])\n",
    "                pipeline_fitted = pipeline.fit(X_train, y_train)\n",
    "                predictions = pipeline_fitted.predict(X_test)\n",
    "                F1 = f1_score(y_test, predictions)\n",
    "                accuracy = accuracy_score(y_test, predictions)\n",
    "                precision = precision_score(y_test, predictions)\n",
    "                recall = recall_score(y_test, predictions)\n",
    "                conf_matrix = confusion_matrix(y_test, predictions)\n",
    "                roc = metrics.roc_curve(y_test, predictions)\n",
    "                results.append([type(dimensionality_reductor).__name__,type(scaler).__name__, type(balancer).__name__, type(classifier).__name__, F1, accuracy, precision, recall, conf_matrix, roc])\n",
    "df_results = pd.DataFrame(results, columns = ['Dimensionality_Reductor', 'Scaler', 'Balancer' ,'Classifier', 'F1', 'Accuracy', 'Precision', 'Recall', 'Confusion_Matrix', 'Roc_Curve'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name the columns of the df with the results\n",
    "df_results = pd.DataFrame(results, columns = ['Dimensionality_Reductor', 'Scaler', 'Balancer' ,'Classifier', 'F1', 'Accuracy', 'Precision', 'Recall', 'Confusion_Matrix', 'Roc_Curve'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort by chosen metruc\n",
    "df_results.sort_values(by=['F1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the table\n",
    "df_results.to_csv('/home/alicja/Documents/furtherModels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement TPE hyperparameter optimization using Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from hyperopt import hp, tpe\n",
    "from hpsklearn import HyperoptEstimator, any_classifier, any_preprocessing, random_forest, xgboost_classification, svc, min_max_scaler, standard_scaler, svc_poly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set hyperopr estimator to choose the best model. Here: chose the best svm classifier with the best preprocessing\n",
    "#Can be use also to chose any_classifier and random_forest\n",
    "%%time\n",
    "estim = HyperoptEstimator(algo=tpe.suggest, classifier = svc('svm'), preprocessing=any_preprocessing('pre'), max_evals=100, trial_timeout=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Hyperopr models\n",
    "%%time\n",
    "estim.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the best hyperopt score. Hyperopr uses accuracy as default score\n",
    "print(estim.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the best model\n",
    "print( estim.best_model() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the best model to fit it to the data and to get other evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler(copy=True, with_mean=False, with_std=False)\n",
    "X = sc.fit_transform(features[:,1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_classifier = SVC(C=77358.56620434714, cache_size=512, class_weight=None,\n",
    "  coef0=5694.538815205853, decision_function_shape='ovr', degree=2.0,\n",
    "  gamma=758.5493658267706, kernel='poly', max_iter=707238803.0,\n",
    "  probability=False, random_state=1, shrinking=True,\n",
    "  tol=1.621850765788926e-05, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the best model to the test set\n",
    "best_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict y on the X_test\n",
    "predictions = best_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the metrics for the best classifier\n",
    "F1 = f1_score(y_test, predictions)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "conf_matrix = confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(F1, accuracy, precision, recall, conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to ge the confusion matric of the classification results\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = [False, True]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [False, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test, predictions, classes)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Optimization of the Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same method as for svm can be used to optimize random forest model\n",
    "%%time\n",
    "estim_rf = HyperoptEstimator(algo=tpe.suggest, classifier = random_forest('random_forest'), preprocessing=any_preprocessing('pre'), max_evals=100, trial_timeout=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Hyperopr models\n",
    "%%time\n",
    "estim_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the best hyperopt score. Hyperopr uses accuracy as default score\n",
    "print(estim_rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the best model\n",
    "print(estim_rf.best_model())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output: {RandomForestClassifier(bootstrap=False, class_weight=None, criterion='entropy', max_depth=None, max_features=0.20603235165277012, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=16, n_jobs=1, oob_score=False, random_state=0, verbose=False, warm_start=False) No preprocessing}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
